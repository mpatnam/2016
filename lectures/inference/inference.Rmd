---
title: "Inference"
output: html_document
---

## Inference 

We are going to have a competition. There is a prize.

You have to guess the proportion of blue beads in this jar. 


![](pics/jar-of-beads.jpg)

You can take a sample (with replacement)
from the jar here:

[https://dgrtwo.shinyapps.io/urn_drawing/](https://dgrtwo.shinyapps.io/urn_drawing/)

Each sample will cost you 10 cents ($0.10). So if you take a sample size of 100 you will have to pay me $10 to collect your prize. 

Once you take your sample, please enter your guess, sample size, and an interval size. We will create an interval of this size with your guess in the middle. If the true percentage is not in your interval you are eliminated. The size of the interval will serve as a tie breaker (smaller wins).

[http://goo.gl/forms/5bv4cRQWKA](http://goo.gl/forms/5bv4cRQWKA)



This chapter introduces the statistical concepts necessary to understand margins of errors, p-values and confidence intervals. These terms are ubiquitous in data science. Let's use polls as an example:


```{r, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(pollstR)

theme_set(theme_bw())

race2012 <- pollstr_polls(topic = '2012-president', after= as.Date("2012-11-3"), max_pages = Inf)

polls <- race2012$questions %>% 
  filter(topic=="2012-president" & state=="US") %>% 
  select(choice, value, margin_of_error, observations, id) %>% 
  filter(choice %in% c("Obama","Romney") & !is.na(margin_of_error)) %>% 
  spread(choice, value) %>% 
  left_join(select(race2012$polls, id, pollster, method), by="id") %>%
  filter(method!="Internet") %>%
  mutate(diff= Obama - Romney) %>%
  select(pollster, diff, Obama, Romney, margin_of_error, observations)

arrange(polls,diff) %>% rename( n=observations) %>% 
  mutate(pollster=ifelse(pollster=="PPP (D-Americans United for Change)","PPP",pollster))
```

What does _margin of error_ mean? The first step is define and understand random variables. 
To do this, we will generate our own imaginary election with 1 million voters of which proportion $p$ are democrats and $1-p$ are republicans. To keep it interesting we will keep generate $p$ at random and not peek.

```{r}
n <- 10^6 ##number of voters
set.seed(1) ##so we all get the same answer
##pick a number between 0.45 and 0.55 (don't peek!):
p <- sample( seq(0.45,0.55,0.001),1) 
x <- rep( c("D","R"), n*c( p, 1-p))
x <- sample(x) ##not necessary, but shuffle them
```

The population is now fixed. There is a true proportion $p$ but we don't know it.

One election day we will do the following to decide who wins (don't ruin the fun by doing it now!):

```{r, eval=FALSE}
prop.table(table(x))
```

Pollsters try to _estimate_ $p$ but asking 1 million people and actually unnecessary so instead. They take a poll. To do this they take a random sample. They pick $N$ random voter phone numbers, call, and ask. Assuming everybody answers and every voter has a phone, we can mimic a random sample of 100 people with:

```{r}
X <- sample(x, 25, replace = TRUE)
```

The pollster then looks at the proportion of democrats in the sample and uses this as a estimate of $p$:

```{r}
prop.table(table(X))
```

So our poll predicts a democrat win! Is this a good _estiamte_? We will see how powerful mathematical statistics is at informing us about exactly how good it is.

Notation: we use lower case $x$ for the population of all voters and capital letters $X$ for the random sample.

## Random Variables

Let's repeat the exercise above a few times. Let's run a few other polls and see what happens:

```{r}
X <- sample(x, 25, replace = TRUE)
mean( X=="D")

X <- sample(x, 25, replace = TRUE)
mean( X=="D")

X <- sample(x, 25, replace = TRUE)
mean( X=="D")

```


Note how the proportion varies. The proportion of democrats is what we call a _random variable_. We are interested in learning of the possible values it can take. To do this will ultimately describe it's distribution. 


### Law of Averages

The law of averages tells us that as the _sample size_ $N$ gets larger our sample average gets closer to closer to the average of the population from which we are sampling. Note that in our case, if we consider the `D` to be 1s and the `R` to be 0s, then the average of `x` is equivalent to the proportion of `D` or $p$. 

To see this, let's repeat the poll with larger and large sample sizes. We introduce the function `sapply`:

```{r}
Ns <- seq(4,100)^2
p_hat <- sapply(Ns, function(N){
  X <- sample(x, N, replace=TRUE)
  mean(X=="D")
})
plot(Ns,p_hat)
abline(h=p)
```

We can see that values start to converge as $N$ gets larger. We also are revealing the value of $p$. 

This law tells us that to predict the election all we need is a very large sample size. In practice, this can be quite expensive. And do we really need a sample size of, say, 10,000 ? Understanding the distribution of the sample average can help us answer this question.


## The Distribution of Random Variables

Because we have access to the entire population, we can actually observe as many values as we want of the population average using a Monte Carlo simulation. Let's do this for a sample size of 1000

```{r}
N <- 1000
B <- 10000
p_hats <- replicate(B, {
  X <- sample(x, N, replace=TRUE)
  mean(X=="D")
})
```

and view the distribution

```{r}
hist(p_hats)
```

We have seen this shape before? Is it the normal distribution?

```{r}
qqnorm(p_hats)
qqline(p_hats)
```

Sure looks like it. We will soon learn the theory that tells us these averages are very well approximated with the normal distribution.

#### Setting the random seed

Before we continue, we briefly explain the following important line of
code:

```{r}
set.seed(1) 
```

Throughout this book, we use random number generators. This implies that many of the results presented can actually change by chance, including the correct answer to problems. One way to ensure that results do not change is by setting R's random number generation seed. For more on the topic please read the help file:

```{r,eval=FALSE}
?set.seed
```
